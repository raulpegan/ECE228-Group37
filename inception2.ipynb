{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lesion_id</th>\n",
       "      <th>image_id</th>\n",
       "      <th>dx</th>\n",
       "      <th>dx_type</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>localization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HAM_0003559</td>\n",
       "      <td>ISIC_0025810</td>\n",
       "      <td>bkl</td>\n",
       "      <td>consensus</td>\n",
       "      <td>75.0</td>\n",
       "      <td>male</td>\n",
       "      <td>face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HAM_0004240</td>\n",
       "      <td>ISIC_0032517</td>\n",
       "      <td>mel</td>\n",
       "      <td>histo</td>\n",
       "      <td>50.0</td>\n",
       "      <td>male</td>\n",
       "      <td>back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HAM_0002610</td>\n",
       "      <td>ISIC_0026876</td>\n",
       "      <td>vasc</td>\n",
       "      <td>consensus</td>\n",
       "      <td>0.0</td>\n",
       "      <td>female</td>\n",
       "      <td>abdomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HAM_0003229</td>\n",
       "      <td>ISIC_0031140</td>\n",
       "      <td>bcc</td>\n",
       "      <td>histo</td>\n",
       "      <td>60.0</td>\n",
       "      <td>male</td>\n",
       "      <td>chest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HAM_0005963</td>\n",
       "      <td>ISIC_0028129</td>\n",
       "      <td>nv</td>\n",
       "      <td>follow_up</td>\n",
       "      <td>45.0</td>\n",
       "      <td>male</td>\n",
       "      <td>foot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lesion_id      image_id    dx    dx_type   age     sex localization\n",
       "0  HAM_0003559  ISIC_0025810   bkl  consensus  75.0    male         face\n",
       "1  HAM_0004240  ISIC_0032517   mel      histo  50.0    male         back\n",
       "2  HAM_0002610  ISIC_0026876  vasc  consensus   0.0  female      abdomen\n",
       "3  HAM_0003229  ISIC_0031140   bcc      histo  60.0    male        chest\n",
       "4  HAM_0005963  ISIC_0028129    nv  follow_up  45.0    male         foot"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_path = './skin-cancer-mnist-ham10000/HAM10000_metadata.csv'\n",
    "metadat = pd.read_csv(metadata_path)\n",
    "num_samples = len(metadat)\n",
    "metadat = metadat.sample(frac=1,random_state=12).reset_index(drop=True) #shuffling data\n",
    "\n",
    "#metadat = metadat['image_id'].add('.jpg')\n",
    "metadat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6409 images belonging to 7 classes.\n",
      "Found 1602 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Use 20% test split (80% training + validation)\n",
    "ntrain2 = int(len(metadat)*0.8)\n",
    "ntrain1 = int(ntrain2*0.8)\n",
    "x_test = metadat.iloc[ntrain2:,:]\n",
    "x_train = metadat.iloc[:ntrain1,:]\n",
    "x_val = metadat.iloc[ntrain1:ntrain2,:]\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './train',\n",
    "        target_size=(299, 299),\n",
    "        batch_size=32,\n",
    "        seed=12)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        './val',\n",
    "        target_size=(299, 299),\n",
    "        batch_size=32,\n",
    "        seed=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.8935 - val_loss: 1.7719\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 4s 600ms/step - loss: 1.8232 - val_loss: 1.7251\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.7856 - val_loss: 1.6895\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.7040 - val_loss: 1.6605\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.6629 - val_loss: 1.6361\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 6s 996ms/step - loss: 1.5816 - val_loss: 1.6142\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.6408 - val_loss: 1.5949\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.6229 - val_loss: 1.5784\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.6224 - val_loss: 1.5641\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.5268 - val_loss: 1.5515\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.5342 - val_loss: 1.5398\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.5931 - val_loss: 1.5294\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.6081 - val_loss: 1.5204\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 6s 985ms/step - loss: 1.4544 - val_loss: 1.5117\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 5s 876ms/step - loss: 1.5130 - val_loss: 1.5034\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.5433 - val_loss: 1.4958\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.5587 - val_loss: 1.4892\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.5626 - val_loss: 1.4829\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.4461 - val_loss: 1.4771\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.4410 - val_loss: 1.4713\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.4813 - val_loss: 1.4660\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.4592 - val_loss: 1.4612\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.4129 - val_loss: 1.4564\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.4003 - val_loss: 1.4519\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.4793 - val_loss: 1.4477\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.4777 - val_loss: 1.4438\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.3781 - val_loss: 1.4401\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.4269 - val_loss: 1.4364\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.3503 - val_loss: 1.4329\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 6s 985ms/step - loss: 1.3428 - val_loss: 1.4295\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.4829 - val_loss: 1.4263\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.4142 - val_loss: 1.4233\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.4356 - val_loss: 1.4205\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.3958 - val_loss: 1.4179\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.3646 - val_loss: 1.4153\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.3753 - val_loss: 1.4129\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 6s 920ms/step - loss: 1.3124 - val_loss: 1.4105\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.4399 - val_loss: 1.4083\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.3202 - val_loss: 1.4062\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 6s 981ms/step - loss: 1.4058 - val_loss: 1.4042\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 5s 895ms/step - loss: 1.3536 - val_loss: 1.4023\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 6s 984ms/step - loss: 1.3836 - val_loss: 1.4004\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 6s 924ms/step - loss: 1.4390 - val_loss: 1.3986\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.3386 - val_loss: 1.3968\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 6s 965ms/step - loss: 1.3076 - val_loss: 1.3950\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 6s 974ms/step - loss: 1.3869 - val_loss: 1.3933\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 6s 1000ms/step - loss: 1.2648 - val_loss: 1.3916\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.3450 - val_loss: 1.3900\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2846 - val_loss: 1.3885\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 6s 921ms/step - loss: 1.2747 - val_loss: 1.3870\n",
      "0 input_2\n",
      "1 conv2d_95\n",
      "2 batch_normalization_95\n",
      "3 activation_95\n",
      "4 conv2d_96\n",
      "5 batch_normalization_96\n",
      "6 activation_96\n",
      "7 conv2d_97\n",
      "8 batch_normalization_97\n",
      "9 activation_97\n",
      "10 max_pooling2d_5\n",
      "11 conv2d_98\n",
      "12 batch_normalization_98\n",
      "13 activation_98\n",
      "14 conv2d_99\n",
      "15 batch_normalization_99\n",
      "16 activation_99\n",
      "17 max_pooling2d_6\n",
      "18 conv2d_103\n",
      "19 batch_normalization_103\n",
      "20 activation_103\n",
      "21 conv2d_101\n",
      "22 conv2d_104\n",
      "23 batch_normalization_101\n",
      "24 batch_normalization_104\n",
      "25 activation_101\n",
      "26 activation_104\n",
      "27 average_pooling2d_10\n",
      "28 conv2d_100\n",
      "29 conv2d_102\n",
      "30 conv2d_105\n",
      "31 conv2d_106\n",
      "32 batch_normalization_100\n",
      "33 batch_normalization_102\n",
      "34 batch_normalization_105\n",
      "35 batch_normalization_106\n",
      "36 activation_100\n",
      "37 activation_102\n",
      "38 activation_105\n",
      "39 activation_106\n",
      "40 mixed0\n",
      "41 conv2d_110\n",
      "42 batch_normalization_110\n",
      "43 activation_110\n",
      "44 conv2d_108\n",
      "45 conv2d_111\n",
      "46 batch_normalization_108\n",
      "47 batch_normalization_111\n",
      "48 activation_108\n",
      "49 activation_111\n",
      "50 average_pooling2d_11\n",
      "51 conv2d_107\n",
      "52 conv2d_109\n",
      "53 conv2d_112\n",
      "54 conv2d_113\n",
      "55 batch_normalization_107\n",
      "56 batch_normalization_109\n",
      "57 batch_normalization_112\n",
      "58 batch_normalization_113\n",
      "59 activation_107\n",
      "60 activation_109\n",
      "61 activation_112\n",
      "62 activation_113\n",
      "63 mixed1\n",
      "64 conv2d_117\n",
      "65 batch_normalization_117\n",
      "66 activation_117\n",
      "67 conv2d_115\n",
      "68 conv2d_118\n",
      "69 batch_normalization_115\n",
      "70 batch_normalization_118\n",
      "71 activation_115\n",
      "72 activation_118\n",
      "73 average_pooling2d_12\n",
      "74 conv2d_114\n",
      "75 conv2d_116\n",
      "76 conv2d_119\n",
      "77 conv2d_120\n",
      "78 batch_normalization_114\n",
      "79 batch_normalization_116\n",
      "80 batch_normalization_119\n",
      "81 batch_normalization_120\n",
      "82 activation_114\n",
      "83 activation_116\n",
      "84 activation_119\n",
      "85 activation_120\n",
      "86 mixed2\n",
      "87 conv2d_122\n",
      "88 batch_normalization_122\n",
      "89 activation_122\n",
      "90 conv2d_123\n",
      "91 batch_normalization_123\n",
      "92 activation_123\n",
      "93 conv2d_121\n",
      "94 conv2d_124\n",
      "95 batch_normalization_121\n",
      "96 batch_normalization_124\n",
      "97 activation_121\n",
      "98 activation_124\n",
      "99 max_pooling2d_7\n",
      "100 mixed3\n",
      "101 conv2d_129\n",
      "102 batch_normalization_129\n",
      "103 activation_129\n",
      "104 conv2d_130\n",
      "105 batch_normalization_130\n",
      "106 activation_130\n",
      "107 conv2d_126\n",
      "108 conv2d_131\n",
      "109 batch_normalization_126\n",
      "110 batch_normalization_131\n",
      "111 activation_126\n",
      "112 activation_131\n",
      "113 conv2d_127\n",
      "114 conv2d_132\n",
      "115 batch_normalization_127\n",
      "116 batch_normalization_132\n",
      "117 activation_127\n",
      "118 activation_132\n",
      "119 average_pooling2d_13\n",
      "120 conv2d_125\n",
      "121 conv2d_128\n",
      "122 conv2d_133\n",
      "123 conv2d_134\n",
      "124 batch_normalization_125\n",
      "125 batch_normalization_128\n",
      "126 batch_normalization_133\n",
      "127 batch_normalization_134\n",
      "128 activation_125\n",
      "129 activation_128\n",
      "130 activation_133\n",
      "131 activation_134\n",
      "132 mixed4\n",
      "133 conv2d_139\n",
      "134 batch_normalization_139\n",
      "135 activation_139\n",
      "136 conv2d_140\n",
      "137 batch_normalization_140\n",
      "138 activation_140\n",
      "139 conv2d_136\n",
      "140 conv2d_141\n",
      "141 batch_normalization_136\n",
      "142 batch_normalization_141\n",
      "143 activation_136\n",
      "144 activation_141\n",
      "145 conv2d_137\n",
      "146 conv2d_142\n",
      "147 batch_normalization_137\n",
      "148 batch_normalization_142\n",
      "149 activation_137\n",
      "150 activation_142\n",
      "151 average_pooling2d_14\n",
      "152 conv2d_135\n",
      "153 conv2d_138\n",
      "154 conv2d_143\n",
      "155 conv2d_144\n",
      "156 batch_normalization_135\n",
      "157 batch_normalization_138\n",
      "158 batch_normalization_143\n",
      "159 batch_normalization_144\n",
      "160 activation_135\n",
      "161 activation_138\n",
      "162 activation_143\n",
      "163 activation_144\n",
      "164 mixed5\n",
      "165 conv2d_149\n",
      "166 batch_normalization_149\n",
      "167 activation_149\n",
      "168 conv2d_150\n",
      "169 batch_normalization_150\n",
      "170 activation_150\n",
      "171 conv2d_146\n",
      "172 conv2d_151\n",
      "173 batch_normalization_146\n",
      "174 batch_normalization_151\n",
      "175 activation_146\n",
      "176 activation_151\n",
      "177 conv2d_147\n",
      "178 conv2d_152\n",
      "179 batch_normalization_147\n",
      "180 batch_normalization_152\n",
      "181 activation_147\n",
      "182 activation_152\n",
      "183 average_pooling2d_15\n",
      "184 conv2d_145\n",
      "185 conv2d_148\n",
      "186 conv2d_153\n",
      "187 conv2d_154\n",
      "188 batch_normalization_145\n",
      "189 batch_normalization_148\n",
      "190 batch_normalization_153\n",
      "191 batch_normalization_154\n",
      "192 activation_145\n",
      "193 activation_148\n",
      "194 activation_153\n",
      "195 activation_154\n",
      "196 mixed6\n",
      "197 conv2d_159\n",
      "198 batch_normalization_159\n",
      "199 activation_159\n",
      "200 conv2d_160\n",
      "201 batch_normalization_160\n",
      "202 activation_160\n",
      "203 conv2d_156\n",
      "204 conv2d_161\n",
      "205 batch_normalization_156\n",
      "206 batch_normalization_161\n",
      "207 activation_156\n",
      "208 activation_161\n",
      "209 conv2d_157\n",
      "210 conv2d_162\n",
      "211 batch_normalization_157\n",
      "212 batch_normalization_162\n",
      "213 activation_157\n",
      "214 activation_162\n",
      "215 average_pooling2d_16\n",
      "216 conv2d_155\n",
      "217 conv2d_158\n",
      "218 conv2d_163\n",
      "219 conv2d_164\n",
      "220 batch_normalization_155\n",
      "221 batch_normalization_158\n",
      "222 batch_normalization_163\n",
      "223 batch_normalization_164\n",
      "224 activation_155\n",
      "225 activation_158\n",
      "226 activation_163\n",
      "227 activation_164\n",
      "228 mixed7\n",
      "229 conv2d_167\n",
      "230 batch_normalization_167\n",
      "231 activation_167\n",
      "232 conv2d_168\n",
      "233 batch_normalization_168\n",
      "234 activation_168\n",
      "235 conv2d_165\n",
      "236 conv2d_169\n",
      "237 batch_normalization_165\n",
      "238 batch_normalization_169\n",
      "239 activation_165\n",
      "240 activation_169\n",
      "241 conv2d_166\n",
      "242 conv2d_170\n",
      "243 batch_normalization_166\n",
      "244 batch_normalization_170\n",
      "245 activation_166\n",
      "246 activation_170\n",
      "247 max_pooling2d_8\n",
      "248 mixed8\n",
      "249 conv2d_175\n",
      "250 batch_normalization_175\n",
      "251 activation_175\n",
      "252 conv2d_172\n",
      "253 conv2d_176\n",
      "254 batch_normalization_172\n",
      "255 batch_normalization_176\n",
      "256 activation_172\n",
      "257 activation_176\n",
      "258 conv2d_173\n",
      "259 conv2d_174\n",
      "260 conv2d_177\n",
      "261 conv2d_178\n",
      "262 average_pooling2d_17\n",
      "263 conv2d_171\n",
      "264 batch_normalization_173\n",
      "265 batch_normalization_174\n",
      "266 batch_normalization_177\n",
      "267 batch_normalization_178\n",
      "268 conv2d_179\n",
      "269 batch_normalization_171\n",
      "270 activation_173\n",
      "271 activation_174\n",
      "272 activation_177\n",
      "273 activation_178\n",
      "274 batch_normalization_179\n",
      "275 activation_171\n",
      "276 mixed9_0\n",
      "277 concatenate_3\n",
      "278 activation_179\n",
      "279 mixed9\n",
      "280 conv2d_184\n",
      "281 batch_normalization_184\n",
      "282 activation_184\n",
      "283 conv2d_181\n",
      "284 conv2d_185\n",
      "285 batch_normalization_181\n",
      "286 batch_normalization_185\n",
      "287 activation_181\n",
      "288 activation_185\n",
      "289 conv2d_182\n",
      "290 conv2d_183\n",
      "291 conv2d_186\n",
      "292 conv2d_187\n",
      "293 average_pooling2d_18\n",
      "294 conv2d_180\n",
      "295 batch_normalization_182\n",
      "296 batch_normalization_183\n",
      "297 batch_normalization_186\n",
      "298 batch_normalization_187\n",
      "299 conv2d_188\n",
      "300 batch_normalization_180\n",
      "301 activation_182\n",
      "302 activation_183\n",
      "303 activation_186\n",
      "304 activation_187\n",
      "305 batch_normalization_188\n",
      "306 activation_180\n",
      "307 mixed9_1\n",
      "308 concatenate_4\n",
      "309 activation_188\n",
      "310 mixed10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 9s 2s/step - loss: 1.3271 - val_loss: 1.4272\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 2s 404ms/step - loss: 1.3369 - val_loss: 1.4167\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 6s 963ms/step - loss: 1.3399 - val_loss: 1.4090\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 5s 876ms/step - loss: 1.3004 - val_loss: 1.3981\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.3016 - val_loss: 1.3928\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.2272 - val_loss: 1.3890\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.3028 - val_loss: 1.3866\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2213 - val_loss: 1.3844\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 5s 889ms/step - loss: 1.2244 - val_loss: 1.3809\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2500 - val_loss: 1.3768\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 6s 997ms/step - loss: 1.4237 - val_loss: 1.3730\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2817 - val_loss: 1.3714\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 6s 999ms/step - loss: 1.2781 - val_loss: 1.3697\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2146 - val_loss: 1.3666\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.3082 - val_loss: 1.3653\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2636 - val_loss: 1.3624\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2597 - val_loss: 1.3601\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.1834 - val_loss: 1.3580\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 6s 940ms/step - loss: 1.1967 - val_loss: 1.3561\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 7s 1s/step - loss: 1.1393 - val_loss: 1.3548\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.1395 - val_loss: 1.3534\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2160 - val_loss: 1.3527\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 6s 956ms/step - loss: 1.2150 - val_loss: 1.3523\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.1338 - val_loss: 1.3511\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2533 - val_loss: 1.3502\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 6s 961ms/step - loss: 1.2936 - val_loss: 1.3493\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2624 - val_loss: 1.3480\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 6s 995ms/step - loss: 1.1916 - val_loss: 1.3471\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 6s 988ms/step - loss: 1.2299 - val_loss: 1.3461\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 6s 937ms/step - loss: 1.1419 - val_loss: 1.3458\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 5s 914ms/step - loss: 1.2483 - val_loss: 1.3453\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.3833 - val_loss: 1.3441\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 6s 964ms/step - loss: 1.2121 - val_loss: 1.3429\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2284 - val_loss: 1.3420\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.1770 - val_loss: 1.3410\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 6s 975ms/step - loss: 1.1682 - val_loss: 1.3403\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 6s 929ms/step - loss: 1.1879 - val_loss: 1.3388\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 6s 969ms/step - loss: 1.3246 - val_loss: 1.3382\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 6s 935ms/step - loss: 1.2278 - val_loss: 1.3370\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 6s 980ms/step - loss: 1.2809 - val_loss: 1.3364\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 6s 934ms/step - loss: 1.1453 - val_loss: 1.3348\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 5s 913ms/step - loss: 1.1940 - val_loss: 1.3339\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2065 - val_loss: 1.3331\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 6s 985ms/step - loss: 1.1119 - val_loss: 1.3332\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.1507 - val_loss: 1.3331\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 6s 957ms/step - loss: 1.1926 - val_loss: 1.3315\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 6s 958ms/step - loss: 1.0372 - val_loss: 1.3310\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 6s 976ms/step - loss: 1.1832 - val_loss: 1.3299\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.1580 - val_loss: 1.3293\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 6s 1s/step - loss: 1.2836 - val_loss: 1.3285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1da7b2e6a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(7, activation='softmax')(x)\n",
    "\n",
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=16/30, amsgrad=False), loss='categorical_crossentropy')\n",
    "\n",
    "# train the model on the new data for a few epochs\n",
    "model.fit_generator(train_generator,\n",
    "        steps_per_epoch=len(train_generator)//32,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator)//32)\n",
    "\n",
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=0.1, decay=16/30, amsgrad=False), loss='categorical_crossentropy')\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit_generator(train_generator,\n",
    "        steps_per_epoch=len(train_generator)//32,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator)//32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./models/epoch50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import binary_accuracy\n",
    "y_pred = model.predict_generator(validation_generator)\n",
    "y_pred = np.argmax(y_pred,axis=0)\n",
    "count = 0\n",
    "correct = 0\n",
    "for i in range(len(validation_generator)):\n",
    "    if np.argmax(validation_generator[i][1]) == y_pred[i]:\n",
    "        correct +=1\n",
    "    count+=1\n",
    "    print(str(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
